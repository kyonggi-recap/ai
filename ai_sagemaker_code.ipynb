{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'facebook/bart-large-cnn',\n",
    "\t'HF_TASK':'summarization'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.37.0',\n",
    "\tpytorch_version='2.1.0',\n",
    "\tpy_version='py310',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")\n",
    "\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'facebook/nllb-200-distilled-600M',\n",
    "\t'HF_TASK':'translation'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.37.0',\n",
    "\tpytorch_version='2.1.0',\n",
    "\tpy_version='py310',\n",
    "\tenv=hub,\n",
    "\trole=role,\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge', # ec2 instance type\n",
    "    src_lang = \"ko\",   # ì†ŒìŠ¤ ì–¸ì–´: í•œêµ­ì–´(ISO 639-3 ì½”ë“œ)\n",
    "    tgt_lang = \"en\",\n",
    ")\n",
    "\n",
    "predictor.predict({'inputs': \"\"\"íŠ¸ëŸ¼í”„ ë¯¸êµ­ ëŒ€í†µë ¹ì´ ìºë‚˜ë‹¤ì™€ ë©•ì‹œì½”ì— ëŒ€í•œ ê´€ì„¸ë¥¼ ì•½ í•œ ë‹¬ê°„ ìœ ì˜ˆí•˜ê¸°ë¡œ í–ˆì§€ë§Œ, ê´€ì„¸ ì •ì±…ì„ ë‘˜ëŸ¬ì‹¼ ë¶ˆí™•ì‹¤ì„±ì´ íˆ¬ì ì‹¬ë¦¬ë¥¼ ì§“ëˆ„ë¥´ë©´ì„œ ë‰´ìš• ì¦ì‹œì˜ 3ëŒ€ ì£¼ê°€ì§€ìˆ˜ê°€ ë™ë°˜ í•˜ë½í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‰´ìš• ì¦ê¶Œ ê±°ë˜ì†Œì—ì„œ ë‚˜ìŠ¤ë‹¥ ì¢…í•© ì§€ìˆ˜ëŠ” ì „ì¥ë³´ë‹¤ 483.48í¬ì¸íŠ¸, 2.61% ì£¼ì €ì•‰ì€ 18,069.26ì— ì¥ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìŠ¤íƒ ë”ë“œ ì•¤ë“œ í‘¸ì–´ìŠ¤ 500ì§€ìˆ˜ëŠ” 104.11í¬ì¸íŠ¸, 1.78% ê¸‰ë½í•œ 5,738.52ì—, ë‹¤ìš°ì¡´ìŠ¤30ì‚°ì—…í‰ê· ì§€ìˆ˜ëŠ” 427.51í¬ì¸íŠ¸, 0.99% ë–¨ì–´ì§„ 42,579.08ë¡œ ê±°ë˜ë¥¼ ë§ˆê°í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŠ” íŠ¸ëŸ¼í”„ê°€ ë¯¸êµ­Â·ë©•ì‹œì½”Â·ìºë‚˜ë‹¤ ë¬´ì—­ í˜‘ì • ì ìš© í’ˆëª©ì— ë‹¤ìŒ ë‹¬ 2ì¼ê¹Œì§€ 25% ê´€ì„¸ë¥¼ ë©´ì œí•˜ê¸°ë¡œ í–ˆì§€ë§Œ, íˆ¬ììë“¤ì€ ì •ì±… ë°©í–¥ì´ ë„ˆë¬´ ê°€ë³€ì ì´ë¼ëŠ” ë° ë¶ˆì•ˆì„ ëŠë‚€ ê²ƒìœ¼ë¡œ í’€ì´ë©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜ í—¤ì§€í€ë“œ ì„¤ë¦½ìë¡œ, íŠ¸ëŸ¼í”„ì˜ ì •ì±…ì— ì œë™ì„ ê±¸ì–´ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëë˜ ìŠ¤ì½§ ë² ì„¼íŠ¸ ì¬ë¬´ì¥ê´€ì´ ìºë‚˜ë‹¤ ì´ë¦¬ë¥¼ ë¹„í•˜í•˜ëŠ” ë“± íŠ¸ëŸ¼í”„ì™€ ìœ ì‚¬í•œ ì–´ë²•ì„ êµ¬ì‚¬í•˜ì ì›”ê°€ëŠ” ì‹¤ë§í•˜ëŠ” ë¶„ìœ„ê¸°ì˜€ìŠµë‹ˆë‹¤.\n",
    "\"\"\",\n",
    "                   'parameters': {'src_lang': 'ko', 'tgt_lang' : 'en'}})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ì–´ -> í•œêµ­ì–´ ë²ˆì—­\n",
    "english_text = \"Trump, the President of the United States, had postponed tariffs on Canada and Mexico for about a month, but the uncertainty surrounding the tariff policy had crushed the investment core, and the New York Stock Exchange's 3rd largest indicator had collapsed along with it.The NASDAQ index on the New York Stock Exchange ended the chapter at 483.48 points above the previous one, 2.61% below the 18,069.26 point.The Standard & Poor's 500 index was 104.11 points, 1.78% below the 5,738.52 point, while Dow Jones's 30 business index closed the deal at 427.51 points, 0.99% below the 42,579.08 point.This led to Trump's decision to exempt from tariffs of 25% on items that applied to US-Mexico-Canada trade until the next 2 days, but investors felt uncertainty that the direction of the\"\n",
    "output = predictor.predict({\n",
    "    'inputs': english_text,\n",
    "    'parameters': {'src_lang': 'eng_Latn', 'tgt_lang': 'kor_Hang'}\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ë„ë„ë“œ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì€ ëª©ìš”ì¼ ë¯¸êµ­ êµìœ¡ë¶€ ì¥ê´€ì„ í•´ì²´í•˜ê¸° ìœ„í•´ í•„ìš”í•œ íˆ¬í‘œë¥¼ í•˜ì§€ ì•ŠìŒì„ ì¸ì •í•˜ê³ , ìˆ˜ì‹­ ë…„ ë™ì•ˆì˜ ë³´ìˆ˜ì ì¸ ì•¼ë§ì„ ì™„ìˆ˜í•˜ê¸° ìœ„í•´ ì„œëª…í–ˆì§€ë§Œ, êµ­ê°€ì— ëŒ€í•œ ìƒˆë¡œìš´ ì˜ë¬¸ì„ ì œê¸°í•˜ê³ , ìˆ˜ë°±ë§Œ ëª…ì˜ ê³µë¦½ í•™êµ, í•™ìƒ ëŒ€ì¶œì£¼ì ë° í•™ë¶€ëª¨ë¥¼ í•´ì²´í•˜ê¸° ìœ„í•´ ì„œëª…í–ˆë‹¤. í˜„ëŒ€ ì—­ì‚¬ì—ì„œ ì–´ë–¤ ëŒ€í†µë ¹ë„ ë‚´ê° ìˆ˜ì¤€ì˜ ê¸°ê´€ì„ íì‡„í•˜ë ¤ê³  ì‹œë„í•˜ì§€ ì•Šì•˜ë‹¤. êµ­ë°©ë¶€ ì¥ê´€ì„ íì‡„í•˜ëŠ” ê²ƒì€ ì˜íšŒì˜ ë²•ìœ¼ë¡œ ì¸í•´ ê¸°ê´€ì„ ì„¤ë¦½í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•©ë‹ˆë‹¤. 1979 ë…„ íŠ¸ëŸ¼í”„ ê´€ë¦¬ìë“¤ì€ ë¯¸êµ­ êµìœ¡ë¶€ ì¥ê´€ì„ í•´ì²´í•˜ê¸° ìœ„í•´ í•„ìš”í•œ íˆ¬í‘œë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ì¸ì •í•˜ê³ , ëŒ€ì‹  íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì€ ëª©ìš”ì¼ êµìœ¡ë¶€ ì¥ê´€ ë§¥ë§ˆí˜¸ì—ê²Œ êµìœ¡ë¶€ ì¥ê´€ì„ ì§€ì‹œí–ˆë‹¤. ê·¸ëŠ” íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ì™€ í•¨ê»˜ êµìœ¡ë¶€ ì¥ê´€ë“¤ì˜ íì‡„ë¥¼ ì˜ˆì¸¡í•˜ê³  êµ­ê°€ì— ëŒ€í•œ ëª¨ë“  í•„ìš” ì‚¬í•­ì„ ì˜ˆì¸¡í•˜ë„ë¡ ì§€ì‹œí–ˆë‹¤. ê·¸ë¦¬ê³  ë™ë°©ì—ì„œ ì—°ì„¤í•˜ê³ , ë™ë°©ì—ì„œ êµì‹¤ì— ì•‰ì•„ìˆëŠ” í•™ìƒë“¤ì˜ ë³´í˜¸êµ¬ì—­ì„ ë³´í˜¸í•˜ê³ , íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ë¶€ ì¥ê´€, íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ë¶€ ì¥ê´€, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ëŠ” íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ë¶€ ì¥ê´€, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ê¸°ê´€ê³¼ í•¨ê»˜ ì¼í•´ì•¼ í•  ê²ƒ, ê·¸ëŸ¬ë‚˜ ê·¸ëŠ” ë¶„ëª…íˆ ë§í–ˆë‹¤. ê·¸ëŠ” ìì‹ ì˜ ì§ë¬´ë¶€ ì¥ê´€, ê·¸ë¦¬ê³  ë‹¤ë¥¸ ê¸°ê´€ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­, ê° êµ­ê°€ë¥¼ í†µì œí•˜ê³ , ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê° ê°êµ­ì— ê±¸ì³, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê°ì´ í¬í•¨í•˜ê³ , ê° êµ­ê°€, ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\n",
    "    task=\"translation\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\",\n",
    "    tokenizer=\"facebook/nllb-200-distilled-600M\",\n",
    "    max_length= 2024\n",
    ")\n",
    "text=\"\"\"President Donald Trump on Thursday signed an executive order to begin dismantling the US Department of Education, seeking to fulfill decades of conservative ambition to get rid of the agency, but raising new questions for the countryâ€™s millions of public schools, student-loan holders and parents.\n",
    "\n",
    "No president in modern history has tried to close down a Cabinet-level agency. Shutting down the department wholesale would require an act of Congress, which created the agency in 1979. Trump officials acknowledge they donâ€™t have the necessary votes to dissolve the department that way; instead, the order Trump signed Thursday instructed Education Secretary Linda McMahon to take â€œall necessary steps to facilitate the closure of the Department of Education and return education authority to the states.â€\n",
    "\n",
    "Speaking in the East Room, surrounded by school children in uniforms sitting at classroom desks, Trump said, â€œWeâ€™re going to shut it down, and shut it down as quickly as possible.â€\n",
    "\n",
    "\n",
    "How precisely the components of the Education Department would be dismantled wasnâ€™t precisely clear.\n",
    "\n",
    "White House press secretary Karoline Leavitt said ahead of Thursdayâ€™s signing that the order would move to â€œgreatly minimize the agency,â€ but that certain â€œcritical functionsâ€ like student loans and administering grants for at-risk students would remain under the agencyâ€™s umbrella.\n",
    "\n",
    "Trump, in his remarks, said those functions would be preserved but â€œredistributed to various other agencies and departments that will take very good care of them.â€\n",
    "\n",
    "The discrepancy could foretell McMahonâ€™s challenge as she works to carry out Trumpâ€™s order. While she has fully endorsed the mission of closing the department, she is also required by law to carry out some of its congressionally mandated functions, like administering loans and providing grants for schools in high-poverty areas.\n",
    "\n",
    "In his remarks, Trump repeatedly cited the imperative of returning control of education to states as his rationale for shuttering the agency, along with what he said were poor test scores compared with high investment-per-student. He said under a new system â€“ without the Education Department â€“ schools could compete with countries in Europe and China, which he said were outcompeting the US.\n",
    "\n",
    "â€œI really believe that theyâ€™ll be as good as any of them,â€ he said. â€œAnd then youâ€™ll have some laggards and weâ€™ll work with them. And we can all tell you who the laggards will be right now.â€\n",
    "\n",
    "Making what he called a â€œpersonal statement,â€ Trump praised teachers as â€œamong the most important people in this country,â€ extending an olive branch even to those represented by labor unions who have decried Trumpâ€™s executive order.\n",
    "\n",
    "â€œI donâ€™t care if theyâ€™re in the union or not in the union. That doesnâ€™t matter. But weâ€™re going to take care of our teachers,â€ he said.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = translator(text, src_lang=\"eng_Latn\", tgt_lang=\"kor_Hang\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ë„ë„ë“œ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì€ ëª©ìš”ì¼ ë¯¸êµ­ êµìœ¡ë¶€ ì¥ê´€ì„ í•´ì²´í•˜ê¸° ìœ„í•´ í•„ìš”í•œ íˆ¬í‘œë¥¼ í•˜ì§€ ì•ŠìŒì„ ì¸ì •í•˜ê³ , ìˆ˜ì‹­ ë…„ ë™ì•ˆì˜ ë³´ìˆ˜ì ì¸ ì•¼ë§ì„ ì™„ìˆ˜í•˜ê¸° ìœ„í•´ ì„œëª…í–ˆì§€ë§Œ, êµ­ê°€ì— ëŒ€í•œ ìƒˆë¡œìš´ ì˜ë¬¸ì„ ì œê¸°í•˜ê³ , ìˆ˜ë°±ë§Œ ëª…ì˜ ê³µë¦½ í•™êµ, í•™ìƒ ëŒ€ì¶œì£¼ì ë° í•™ë¶€ëª¨ë¥¼ í•´ì²´í•˜ê¸° ìœ„í•´ ì„œëª…í–ˆë‹¤. í˜„ëŒ€ ì—­ì‚¬ì—ì„œ ì–´ë–¤ ëŒ€í†µë ¹ë„ ë‚´ê° ìˆ˜ì¤€ì˜ ê¸°ê´€ì„ íì‡„í•˜ë ¤ê³  ì‹œë„í•˜ì§€ ì•Šì•˜ë‹¤. êµ­ë°©ë¶€ ì¥ê´€ì„ íì‡„í•˜ëŠ” ê²ƒì€ ì˜íšŒì˜ ë²•ìœ¼ë¡œ ì¸í•´ ê¸°ê´€ì„ ì„¤ë¦½í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•©ë‹ˆë‹¤. 1979 ë…„ íŠ¸ëŸ¼í”„ ê´€ë¦¬ìë“¤ì€ ë¯¸êµ­ êµìœ¡ë¶€ ì¥ê´€ì„ í•´ì²´í•˜ê¸° ìœ„í•´ í•„ìš”í•œ íˆ¬í‘œë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ì¸ì •í•˜ê³ , ëŒ€ì‹  íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì€ ëª©ìš”ì¼ êµìœ¡ë¶€ ì¥ê´€ ë§¥ë§ˆí˜¸ì—ê²Œ êµìœ¡ë¶€ ì¥ê´€ì„ ì§€ì‹œí–ˆë‹¤. ê·¸ëŠ” íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ì™€ í•¨ê»˜ êµìœ¡ë¶€ ì¥ê´€ë“¤ì˜ íì‡„ë¥¼ ì˜ˆì¸¡í•˜ê³  êµ­ê°€ì— ëŒ€í•œ ëª¨ë“  í•„ìš” ì‚¬í•­ì„ ì˜ˆì¸¡í•˜ë„ë¡ ì§€ì‹œí–ˆë‹¤. ê·¸ë¦¬ê³  ë™ë°©ì—ì„œ ì—°ì„¤í•˜ê³ , ë™ë°©ì—ì„œ êµì‹¤ì— ì•‰ì•„ìˆëŠ” í•™ìƒë“¤ì˜ ë³´í˜¸êµ¬ì—­ì„ ë³´í˜¸í•˜ê³ , íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ë¶€ ì¥ê´€, íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ë¶€ ì¥ê´€, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ëŠ” íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ì§ë¬´ë¶€ ì¥ê´€, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ê¸°ê´€ê³¼ í•¨ê»˜ ì¼í•´ì•¼ í•  ê²ƒ, ê·¸ëŸ¬ë‚˜ ê·¸ëŠ” ë¶„ëª…íˆ ë§í–ˆë‹¤. ê·¸ëŠ” ìì‹ ì˜ ì§ë¬´ë¶€ ì¥ê´€, ê·¸ë¦¬ê³  ë‹¤ë¥¸ ê¸°ê´€ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­ ë‚´ê°ì—ì„œ, ê·¸ë¦¬ê³  ë¯¸êµ­, ê° êµ­ê°€ë¥¼ í†µì œí•˜ê³ , ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê·¸ë¦¬ê³  ë¯¸êµ­, ê° ê°êµ­ì— ê±¸ì³, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê° êµ­ê°€, ê°ì´ í¬í•¨í•˜ê³ , ê° êµ­ê°€, ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³ , ê°ì´ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.'}]\n"
     ]
    }
   ],
   "source": [
    "result = translator(text, src_lang=\"eng_Latn\", tgt_lang=\"kor_Hang\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m275.9/275.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Requirement already satisfied: tqdm in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: Pillow in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: scipy in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sentence_transformers) (0.29.3)\n",
      "Requirement already satisfied: requests in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: filelock in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dongohkang/myenv1/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Installing collected packages: tokenizers, transformers, sentence_transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.0\n",
      "    Uninstalling transformers-4.37.0:\n",
      "      Successfully uninstalled transformers-4.37.0\n",
      "Successfully installed sentence_transformers-3.4.1 tokenizers-0.21.1 transformers-4.49.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/dongohkang/myenv1/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/trainer_callback.py\", line 28, in <module>\n",
      "    from .trainer_utils import HPSearchBackend, IntervalStrategy, SaveStrategy, has_length\n",
      "ImportError: cannot import name 'SaveStrategy' from 'transformers.trainer_utils' (/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/trainer_utils.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/1x/1jr0n7yx0fq1gbn6d8p02pdc0000gn/T/ipykernel_50023/2336861096.py\", line 2, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/__init__.py\", line 14, in <module>\n",
      "    from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/cross_encoder/__init__.py\", line 3, in <module>\n",
      "    from .CrossEncoder import CrossEncoder\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py\", line 18, in <module>\n",
      "    from sentence_transformers.evaluation.SentenceEvaluator import SentenceEvaluator\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/evaluation/__init__.py\", line 9, in <module>\n",
      "    from .NanoBEIREvaluator import NanoBEIREvaluator\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/evaluation/NanoBEIREvaluator.py\", line 11, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py\", line 33, in <module>\n",
      "    from sentence_transformers.model_card import SentenceTransformerModelCardData, generate_model_card\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/sentence_transformers/model_card.py\", line 24, in <module>\n",
      "    from transformers import TrainerCallback\n",
      "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\n",
      "    {0} requires the ğŸ¤— Tokenizers library but it was not found in your environment. You can install it with:\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\n",
      "    # docstyle-ignore\n",
      "RuntimeError: Failed to import transformers.trainer_callback because of the following error (look up to see its traceback):\n",
      "cannot import name 'SaveStrategy' from 'transformers.trainer_utils' (/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/trainer_utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1192, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.deta.configuration_deta'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/1x/1jr0n7yx0fq1gbn6d8p02pdc0000gn/T/ipykernel_50023/1408883618.py\", line 17, in <module>\n",
      "    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 543, in from_pretrained\n",
      "    has_local_code = type(config) in cls._model_mapping.keys()\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 757, in keys\n",
      "    def __getitem__(self, key):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 758, in <listcomp>\n",
      "    if key in self._extra_content:\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 754, in _load_attr_from_module\n",
      "    common_keys = set(self._config_mapping.keys()).intersection(self._model_mapping.keys())\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 698, in getattribute_from_module\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\n",
      "    {0} requires the ğŸ¤— Tokenizers library but it was not found in your environment. You can install it with:\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\n",
      "    # docstyle-ignore\n",
      "RuntimeError: Failed to import transformers.models.deta.configuration_deta because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.deta.configuration_deta'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/dongohkang/myenv1/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.6846, 0.6126],\n",
      "        [0.6846, 1.0000, 0.6558],\n",
      "        [0.6126, 0.6558, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "revision = None  # Replace with the specific revision to ensure reproducibility if the model is updated.\n",
    "\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", revision=revision)\n",
    "\n",
    "texts = [\n",
    "    \"Illustration of the REaLTabFormer model. The left block shows the non-relational tabular data model using GPT-2 with a causal LM head. In contrast, the right block shows how a relational dataset's child table is modeled using a sequence-to-sequence (Seq2Seq) model. The Seq2Seq model uses the observations in the parent table to condition the generation of the observations in the child table. The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model.\",\n",
    "    \"Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.\",\n",
    "    \"As the economies of Southeast Asia continue adopting digital technologies, policy makers increasingly ask how to prepare the workforce for emerging labor demands. However, little is known about the skills that workers need to adapt to these changes\"\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine-similarity for each pair of sentences\n",
    "scores = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1)\n",
    "\n",
    "print(scores.cpu())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë°°í¬\n",
    "\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'avsolatorio/GIST-small-Embedding-v0'\n",
    "}\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface-tei\",version=\"1.2.3\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.xlarge\",\n",
    "  )\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#endpoint í˜¸ì¶œ\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# SageMaker ëŸ°íƒ€ì„ í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = boto3.client('sagemaker-runtime', region_name='ap-northeast-2')  # ë¦¬ì „ì€ ì‹¤ì œ ë°°í¬ ë¦¬ì „ìœ¼ë¡œ ë³€ê²½\n",
    "\n",
    "# ì—”ë“œí¬ì¸íŠ¸ ì´ë¦„\n",
    "endpoint_name = 'tei-2025-03-21-11-29-55-250'\n",
    "\n",
    "# ìš”ì²­ ë°ì´í„° (JSON í˜•ì‹)\n",
    "payload = json.dumps({\n",
    "    \"inputs\": [\n",
    "        \"\"\"Illustration of the REaLTabFormer model. The left block shows the non-relational tabular data model using GPT-2 with a causal LM head. In contrast, the right block shows how a relational dataset's child table is modeled using a sequence-to-sequence (Seq2Seq) model. The Seq2Seq model uses the observations in the parent table to condition the generation of the observations in the child table. The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model.\"\"\",\n",
    "        \"\"\"Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.\"\"\",\n",
    "        \"\"\"As the economies of Southeast Asia continue adopting digital technologies, policy makers increasingly ask how to prepare the workforce for emerging labor demands. However, little is known about the skills that workers need to adapt to these changes\"\"\"\n",
    "    ]\n",
    "})\n",
    "# ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "# ì‘ë‹µ ì²˜ë¦¬\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# ì„ë² ë”© ì¶”ì¶œ (resultëŠ” ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ í›„ ë°›ì€ ì‘ë‹µ)\n",
    "embedding1 = np.array(result[0])  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©\n",
    "embedding2 = np.array(result[1])  # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©\n",
    "embedding3 = np.array(result[2])  # ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©\n",
    "\n",
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (1 - ì½”ì‚¬ì¸ ê±°ë¦¬)\n",
    "similarity_1_2 = 1 - cosine(embedding1, embedding2)\n",
    "similarity_1_3 = 1 - cosine(embedding1, embedding3)\n",
    "similarity_2_3 = 1 - cosine(embedding2, embedding3)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"í…ìŠ¤íŠ¸ 1ê³¼ í…ìŠ¤íŠ¸ 2 ê°„ ìœ ì‚¬ë„: {similarity_1_2}\")\n",
    "print(f\"í…ìŠ¤íŠ¸ 1ê³¼ í…ìŠ¤íŠ¸ 3 ê°„ ìœ ì‚¬ë„: {similarity_1_3}\")\n",
    "print(f\"í…ìŠ¤íŠ¸ 2ì™€ í…ìŠ¤íŠ¸ 3 ê°„ ìœ ì‚¬ë„: {similarity_2_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar sentence is: Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.\n",
      "Similarity score: 0.8502\n",
      "\n",
      "All similarity scores:\n",
      "Sentence 1: 0.7494 - Illustration of the REaLTabFormer model. The left ...\n",
      "Sentence 2: 0.8502 - Predicting human mobility holds significant practi...\n",
      "Sentence 3: 0.7160 - As the economies of Southeast Asia continue adopti...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "revision = None  # Replace with a specific revision if needed for reproducibility\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", revision=revision)\n",
    "\n",
    "# Your list of 10 sentences (for this example, Iâ€™ll use the 3 you provided; expand to 10 as needed)\n",
    "texts = [\n",
    "    \"Illustration of the REaLTabFormer model. The left block shows the non-relational tabular data model using GPT-2 with a causal LM head. In contrast, the right block shows how a relational dataset's child table is modeled using a sequence-to-sequence (Seq2Seq) model. The Seq2Seq model uses the observations in the parent table to condition the generation of the observations in the child table. The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model.\",\n",
    "    \"Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.\",\n",
    "    \"As the economies of Southeast Asia continue adopting digital technologies, policy makers increasingly ask how to prepare the workforce for emerging labor demands. However, little is known about the skills that workers need to adapt to these changes\"\n",
    "    \n",
    "    # Add more sentences here to make it 10 if needed\n",
    "]\n",
    "\n",
    "# Assume you fetch liked sentences from the database (replace with your actual database query)\n",
    "liked_texts = [\n",
    "    \"I enjoy reading about advanced machine learning models and their applications.\",\n",
    "    \"Human mobility prediction is fascinating, especially when using transformer-based models.\"\n",
    "    # Add more liked sentences as retrieved from your DB\n",
    "]\n",
    "\n",
    "# Compute embeddings for liked texts\n",
    "liked_embeddings = model.encode(liked_texts, convert_to_tensor=True)\n",
    "\n",
    "# Compute the mean embedding of liked texts (centroid)\n",
    "mean_liked_embedding = torch.mean(liked_embeddings, dim=0)\n",
    "\n",
    "# Compute embeddings for the 10 sentences\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities between each of the 10 embeddings and the mean liked embedding\n",
    "# Since mean_liked_embedding is 1D and embeddings is 2D, we unsqueeze to make it broadcastable\n",
    "similarities = F.cosine_similarity(embeddings, mean_liked_embedding.unsqueeze(0), dim=1)\n",
    "\n",
    "# Find the index of the sentence with the highest similarity\n",
    "most_similar_index = torch.argmax(similarities).item()\n",
    "\n",
    "# Get the most similar sentence\n",
    "most_similar_sentence = texts[most_similar_index]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The most similar sentence is: {most_similar_sentence}\")\n",
    "print(f\"Similarity score: {similarities[most_similar_index].item():.4f}\")\n",
    "\n",
    "# Optional: Print all similarities for reference\n",
    "print(\"\\nAll similarity scores:\")\n",
    "for i, (text, score) in enumerate(zip(texts, similarities)):\n",
    "    print(f\"Sentence {i+1}: {score.item():.4f} - {text[:50]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
